---
title: "The Attention Mechanism and GPT's Transformer Architecture Behind LLMs and something about Graph Neural Network (GNN)"
date: 2026-01-15T23:23:57-06:00
draft: false
---

This quarter I'm taking Professor Candogan's *Frontiers in AI for Operational Decision-Making* (BUSN 40915). We swept through the fundamentals of AI for the first two weeks. My opinion may be biased because I am already preloaded with all the  AI terminologies, but the course infused extreme clarity into my understanding.

The following are key slides that I found informative/inspiring. Courtesy to Professor Candogan — I don't own copyrights! All mistakes are on my own.

Something about GNN: it's essentially a generalization of CNN.

{{<figure align="center" width="100%" src="https://raw.githubusercontent.com/aritang/aritangPictures/main/static/online/GNN.jpeg" caption="">}}

Something about Transformer: (I tend to think...) it's a much more powerful version of RNN:

{{<figure align="center" width="100%" src="https://raw.githubusercontent.com/aritang/aritangPictures/main/static/online/transformerAttention.jpeg" caption="">}}
