<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Restless Multi-Armed Bandits | Primal, Dual and Opportunity Costs | ariana&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="The textbook Multi-armed Bandit Allocation Indices by John Gittins, Kevin Glazebrook, Richard Weber has a charming Chapter 6, titled &ldquo;Restless bandits and Lagrangian relaxation&rdquo;. But the authors skipped a step in the prove of how to go from the LP to Dual and to Whittle Index Policy.
Prelims:

An RMAB (Restless Multi-Armed Bandit) instance is defined through the tuple
$$
\lang S, A, r_{i}, \mathcal P_{i} \rang_{i \in [N]}
$$
where $S_i $ is the state space, $A_i = \lbrace 0, 1 \rbrace$ the action space, $r_{i}$ and $\mathcal P_{i}$ denote the reward and transition kernels arm $i$.">
<meta name="author" content="">
<link rel="canonical" href="//localhost:1313/posts/rmab_dual/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="//localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/posts/rmab_dual/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>




<meta property="og:title" content="Restless Multi-Armed Bandits | Primal, Dual and Opportunity Costs" />
<meta property="og:description" content="The textbook Multi-armed Bandit Allocation Indices by John Gittins, Kevin Glazebrook, Richard Weber has a charming Chapter 6, titled &ldquo;Restless bandits and Lagrangian relaxation&rdquo;. But the authors skipped a step in the prove of how to go from the LP to Dual and to Whittle Index Policy.
Prelims:

An RMAB (Restless Multi-Armed Bandit) instance is defined through the tuple
$$
\lang S, A, r_{i}, \mathcal P_{i} \rang_{i \in [N]}
$$
where $S_i $ is the state space, $A_i = \lbrace 0, 1 \rbrace$ the action space, $r_{i}$ and $\mathcal P_{i}$ denote the reward and transition kernels arm $i$." />
<meta property="og:type" content="article" />
<meta property="og:url" content="//localhost:1313/posts/rmab_dual/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-12-22T00:39:38+08:00" />
<meta property="article:modified_time" content="2024-12-22T00:39:38+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Restless Multi-Armed Bandits | Primal, Dual and Opportunity Costs"/>
<meta name="twitter:description" content="The textbook Multi-armed Bandit Allocation Indices by John Gittins, Kevin Glazebrook, Richard Weber has a charming Chapter 6, titled &ldquo;Restless bandits and Lagrangian relaxation&rdquo;. But the authors skipped a step in the prove of how to go from the LP to Dual and to Whittle Index Policy.
Prelims:

An RMAB (Restless Multi-Armed Bandit) instance is defined through the tuple
$$
\lang S, A, r_{i}, \mathcal P_{i} \rang_{i \in [N]}
$$
where $S_i $ is the state space, $A_i = \lbrace 0, 1 \rbrace$ the action space, $r_{i}$ and $\mathcal P_{i}$ denote the reward and transition kernels arm $i$."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "//localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Restless Multi-Armed Bandits | Primal, Dual and Opportunity Costs",
      "item": "//localhost:1313/posts/rmab_dual/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Restless Multi-Armed Bandits | Primal, Dual and Opportunity Costs",
  "name": "Restless Multi-Armed Bandits | Primal, Dual and Opportunity Costs",
  "description": "The textbook Multi-armed Bandit Allocation Indices by John Gittins, Kevin Glazebrook, Richard Weber has a charming Chapter 6, titled \u0026ldquo;Restless bandits and Lagrangian relaxation\u0026rdquo;. But the authors skipped a step in the prove of how to go from the LP to Dual and to Whittle Index Policy.\nPrelims:\nAn RMAB (Restless Multi-Armed Bandit) instance is defined through the tuple $$ \\lang S, A, r_{i}, \\mathcal P_{i} \\rang_{i \\in [N]} $$ where $S_i $ is the state space, $A_i = \\lbrace 0, 1 \\rbrace$ the action space, $r_{i}$ and $\\mathcal P_{i}$ denote the reward and transition kernels arm $i$.\n",
  "keywords": [
    
  ],
  "articleBody": "The textbook Multi-armed Bandit Allocation Indices by John Gittins, Kevin Glazebrook, Richard Weber has a charming Chapter 6, titled “Restless bandits and Lagrangian relaxation”. But the authors skipped a step in the prove of how to go from the LP to Dual and to Whittle Index Policy.\nPrelims:\nAn RMAB (Restless Multi-Armed Bandit) instance is defined through the tuple $$ \\lang S, A, r_{i}, \\mathcal P_{i} \\rang_{i \\in [N]} $$ where $S_i $ is the state space, $A_i = \\lbrace 0, 1 \\rbrace$ the action space, $r_{i}$ and $\\mathcal P_{i}$ denote the reward and transition kernels arm $i$.\nGiven the budget constraints, the decision make seek the policy $\\pi: S \\mapsto A$ that maximizes long-term average reward subject to budget constraints: If the budget constraint is “soft”—i.e. on average the actions don’t exceed a given budget: $$ \\lim_{T\\to \\infty} \\frac 1T\\mathbb E_{a\\sim \\pi(\\cdot)}[\\sum_{t = 1}^T\\sum_{i \\in [N]} a_i^t]\\le B. $$\nGiven an RMAB instance, we can solve the optimal soft-budget policy in the form of the following occupancy-measure LP: $$ \\begin{align*} \\max_{\\mu} \\ \u0026 \\sum_{i\\in [N]} \\sum_{s_i, a_i} \\mu_i(s_i, a_i) r_i(s_i, a_i)\\cr s.t.\\ \u0026 \\sum_{s_i, a_i} P[s_i\\to s_i’| a_i] \\mu_i(s_i, a_i)= \\sum_{a_i}\\mu_i(s_i’, a_i), \\forall s_i’, i\\cr \u0026 \\sum_{a_i, s_i}\\mu_i(s_i, a_i) = 1, \\forall i\\in [N]\\cr \u0026 \\sum_i \\sum_{s_i}\\mu_i(s_i, 1)\\le B\\cr \u0026 \\mu_i(s_i, a_i) \\ge 0, \\forall i, s_i, a_i. \\end{align*} $$ Let $V_i(s_i),\\nu_i, \\rho$ be the dual variables associated with the above three (groups) of constraints. The dual writes: $$ \\begin{align*} \\min_{\\nu_i, \\rho}\u0026 \\sum_{i\\in [N]} \\nu_i + \\rho B\\cr \\text{subject to:} \u0026 \\cr V_i(s_i) + \\nu_i \u0026 \\ge r_i(s_i, a_i) - \\mathbb I\\lbrace a_i = 1\\rbrace \\rho + \\sum_{s_i’}V_i(s_i’)P[s_i\\to s_i’\\mid a_i],\\forall i, s_i\\cr \\rho \u0026 \\ge 0. \\end{align*} $$ Notice by complementary slackness, we have, for optimal dual solutions $\\nu_i^\\star, V_i^\\star(\\cdot)$, if a state $s_i$ is ever visited (i.e. $\\mu_i(s_i, 0) + \\mu_i(s_i, 1) \u003e 0$), then, either $$ V_i(s_i) + \\nu_i = r_i(s_i, 1) - \\rho + \\sum_{s_i’}V_i(s_i’)P[s_i\\to s_i’\\mid a_i=1], $$ or $$ V_i(s_i) + \\nu_i = r_i(s_i, 0) + \\sum_{s_i’}V_i(s_i’)P[s_i\\to s_i’\\mid a_i = 0]. $$ Which can be written in compact form, but more intuitive:\nI think the intuitive here is beautiful. If you formulate a constrained-resource-allocation-type problem directly as linear programming (or MIP-relaxed), its dual variables often resemble “shadow prices”, or, simply, opportunity costs (e.g. try knapsack). It’s similar here.\nThe textbook’s section 6.3 breezes past this with, ‘It is interesting to see how (6.6) can be obtained from (6.1)–(6.4)…’\nInteresting? Absolutely. Obvious? Oh, totally—if you squint hard enough!\n",
  "wordCount" : "412",
  "inLanguage": "en",
  "datePublished": "2024-12-22T00:39:38+08:00",
  "dateModified": "2024-12-22T00:39:38+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/posts/rmab_dual/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ariana's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/favicon.ico"
    }
  }
}
</script>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Product+Sans:wght@400;700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="//localhost:1313/css/custom.b017fc6c61ff4eacd2943e3c9726e1a935c706d1ea1a1b7c17773e63b47342d3.css">
    
<script async src="https://www.googletagmanager.com/gtag/js?id=AW-16651897053">
</script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'AW-16651897053');
</script>

<script>
  gtag('event', 'conversion', {
      'send_to': 'AW-16651897053/wjItCK3FisUZEN2Rn4Q-',
      'value': 1.0,
      'currency': 'USD'
  });
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=AW-16651897053"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'AW-16651897053');
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="ariana&#39;s blog (Alt + H)">ariana&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/search" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/archives" title="archive">
                    <span>archive</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/posts/tags" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/posts/pinned" title="about me">
                    <span>about me</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/procrastination_bulletin" title="pending ideas">
                    <span>pending ideas</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Restless Multi-Armed Bandits | Primal, Dual and Opportunity Costs
    </h1>
    <div class="post-meta">&lt;span title=&#39;2024-12-22 00:39:38 &#43;0800 CST&#39;&gt;December 22, 2024&lt;/span&gt;

</div>
  </header>
  <div class="post-content"><p>The textbook <em>Multi-armed Bandit Allocation Indices</em> by John Gittins, Kevin Glazebrook, Richard Weber has a charming Chapter 6, titled &ldquo;<em><strong>Restless bandits and Lagrangian relaxation</strong></em>&rdquo;. But the authors skipped a step in the prove of how to go from the LP to Dual and to Whittle Index Policy.</p>
<p>Prelims:</p>
<blockquote>
<p>An RMAB (Restless Multi-Armed Bandit) instance is defined through the tuple
$$
\lang S, A, r_{i}, \mathcal P_{i} \rang_{i \in [N]}
$$
where $S_i $ is the state space, $A_i = \lbrace 0, 1 \rbrace$ the action space, $r_{i}$ and $\mathcal P_{i}$ denote the reward and transition kernels arm $i$.</p>
<p>Given the budget constraints, the decision make seek the policy $\pi: S \mapsto A$ that maximizes long-term average reward subject to budget constraints: <figure class="align-center ">
    <img loading="lazy" src="/art/rmab_eq_1.jpeg#center" width="100%"/> 
</figure>
</p>
<p>If the budget constraint is &ldquo;soft&rdquo;—i.e. on average the actions don&rsquo;t exceed a given budget:
$$
\lim_{T\to \infty} \frac 1T\mathbb E_{a\sim \pi(\cdot)}[\sum_{t = 1}^T\sum_{i \in [N]} a_i^t]\le  B.
$$</p>
</blockquote>
<p>Given an RMAB instance, we can solve the optimal soft-budget policy in the form of the following occupancy-measure LP:
$$
\begin{align*}
\max_{\mu} \ &amp; \sum_{i\in [N]} \sum_{s_i, a_i} \mu_i(s_i, a_i) r_i(s_i, a_i)\cr
s.t.\  &amp; \sum_{s_i, a_i} P[s_i\to s_i&rsquo;| a_i] \mu_i(s_i, a_i)= \sum_{a_i}\mu_i(s_i&rsquo;, a_i), \forall s_i&rsquo;, i\cr
&amp; \sum_{a_i, s_i}\mu_i(s_i, a_i) = 1, \forall i\in [N]\cr
&amp; \sum_i \sum_{s_i}\mu_i(s_i, 1)\le B\cr
&amp; \mu_i(s_i, a_i) \ge 0, \forall i, s_i, a_i.
\end{align*}
$$
Let $V_i(s_i),\nu_i, \rho$ be the dual variables associated with the above three (groups) of constraints. The dual writes:
$$
\begin{align*}
\min_{\nu_i, \rho}&amp; \sum_{i\in [N]} \nu_i + \rho B\cr
\text{subject to:} &amp; \cr
V_i(s_i) + \nu_i &amp; \ge r_i(s_i, a_i) - \mathbb I\lbrace a_i = 1\rbrace \rho + \sum_{s_i&rsquo;}V_i(s_i&rsquo;)P[s_i\to s_i&rsquo;\mid a_i],\forall i, s_i\cr
\rho &amp; \ge 0.
\end{align*}
$$
Notice by complementary slackness, we have, for optimal dual solutions $\nu_i^\star, V_i^\star(\cdot)$, if a state $s_i$ is ever visited (i.e. $\mu_i(s_i, 0) + \mu_i(s_i, 1) &gt; 0$), then, either
$$
V_i(s_i) + \nu_i  = r_i(s_i, 1) - \rho + \sum_{s_i&rsquo;}V_i(s_i&rsquo;)P[s_i\to s_i&rsquo;\mid a_i=1],
$$
or
$$
V_i(s_i) + \nu_i  = r_i(s_i, 0) + \sum_{s_i&rsquo;}V_i(s_i&rsquo;)P[s_i\to s_i&rsquo;\mid a_i = 0].
$$
Which can be written in compact form, but more intuitive:</p>
<figure class="align-center ">
    <img loading="lazy" src="/art/rmab_eq_2.jpeg#center" width="88%"/> 
</figure>

<p>I think the intuitive here is beautiful. If you formulate a constrained-resource-allocation-type problem directly as linear programming (or MIP-relaxed), its dual variables often resemble &ldquo;shadow prices&rdquo;, or, simply, opportunity costs (e.g. try knapsack). It&rsquo;s similar here.</p>
<p>The textbook&rsquo;s section 6.3 breezes past this with, &lsquo;It is <em><strong>interesting</strong></em> to see how (6.6) can be obtained from (6.1)–(6.4)&hellip;&rsquo;</p>
<figure class="align-center ">
    <img loading="lazy" src="/art/RMAB_skip.jpeg#center"
         alt="Interesting? Absolutely. Obvious? Oh, totally—if you squint hard enough!" width="100%"/> <figcaption>
            <p>Interesting? Absolutely. Obvious? Oh, totally—if you squint hard enough!</p>
        </figcaption>
</figure>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="//localhost:1313/">ariana&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script id="usercentrics-cmp" src="https://web.cmp.usercentrics.eu/ui/loader.js" data-settings-id="_KlbvhaKistzRA" async></script>

</body>

</html>
