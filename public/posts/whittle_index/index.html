<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>RMAB and Whittle index | a modern approach to decision-making | ariana&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Decision-making in dynamic environments over time presents unique challenges, particularly when the conditions influencing decisions are constantly changing. In such scenarios, traditional decision-making models often fall short. This is where Restless Multi-Armed Bandits (RMAB) come into play. The method provides a robust framework for modeling and optimizing decisions over time. Here&rsquo;s a brief introduction of the concept of RMAB and the Whittle Index Policy.
understanding Restless Multi-Armed Bandits what are RMAB?">
<meta name="author" content="">
<link rel="canonical" href="/posts/whittle_index/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" href="apple-touch-icon.png">
<link rel="mask-icon" href="safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>




<meta property="og:title" content="RMAB and Whittle index | a modern approach to decision-making" />
<meta property="og:description" content="Decision-making in dynamic environments over time presents unique challenges, particularly when the conditions influencing decisions are constantly changing. In such scenarios, traditional decision-making models often fall short. This is where Restless Multi-Armed Bandits (RMAB) come into play. The method provides a robust framework for modeling and optimizing decisions over time. Here&rsquo;s a brief introduction of the concept of RMAB and the Whittle Index Policy.
understanding Restless Multi-Armed Bandits what are RMAB?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/whittle_index/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-06-05T23:56:59+08:00" />
<meta property="article:modified_time" content="2024-06-05T23:56:59+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RMAB and Whittle index | a modern approach to decision-making"/>
<meta name="twitter:description" content="Decision-making in dynamic environments over time presents unique challenges, particularly when the conditions influencing decisions are constantly changing. In such scenarios, traditional decision-making models often fall short. This is where Restless Multi-Armed Bandits (RMAB) come into play. The method provides a robust framework for modeling and optimizing decisions over time. Here&rsquo;s a brief introduction of the concept of RMAB and the Whittle Index Policy.
understanding Restless Multi-Armed Bandits what are RMAB?"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "RMAB and Whittle index | a modern approach to decision-making",
      "item": "/posts/whittle_index/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "RMAB and Whittle index | a modern approach to decision-making",
  "name": "RMAB and Whittle index | a modern approach to decision-making",
  "description": "Decision-making in dynamic environments over time presents unique challenges, particularly when the conditions influencing decisions are constantly changing. In such scenarios, traditional decision-making models often fall short. This is where Restless Multi-Armed Bandits (RMAB) come into play. The method provides a robust framework for modeling and optimizing decisions over time. Here\u0026rsquo;s a brief introduction of the concept of RMAB and the Whittle Index Policy.\nunderstanding Restless Multi-Armed Bandits what are RMAB?",
  "keywords": [
    
  ],
  "articleBody": "Decision-making in dynamic environments over time presents unique challenges, particularly when the conditions influencing decisions are constantly changing. In such scenarios, traditional decision-making models often fall short. This is where Restless Multi-Armed Bandits (RMAB) come into play. The method provides a robust framework for modeling and optimizing decisions over time. Here’s a brief introduction of the concept of RMAB and the Whittle Index Policy.\nunderstanding Restless Multi-Armed Bandits what are RMAB? The multi-armed bandit problem is a classic version of the problem of optimal allocation of activity under certainty: simply saying, one has $n$ projects, the state of project $i\\in [n]$ being denoted by $x_i(t)$​.\nOne can operate only one project at once: if one operates project $i$ then one receives reward $g_i(x_i(t))$ in the time-interval $(t, t + 1)$ and the transition $x_i(t)\\rightarrow x_i(t + 1)$ follows a Markov rule specific to project $i$. The unused projects neither yield reward nor change state; current states of all projects are known at any time.\nThe problem is to so choose the project at each moment that the expected discounted reward over an infinite future is maximal.\nRestless Multi-Armed Bandits extend this classic multi-armed bandit problem to scenarios where all projects (or, ‘arms’) evolve over time, regardless of whether they are chosen, and might incur reward regardless of if being activated. Moreover, one might be able to pull $B$ arms at any time. This dynamic nature makes RMAB particularly applicable to real-world decision-making processes, where neglecting any option can still lead to changes in its state.\nexample Consider a platform coordinating volunteers for various tasks, where each volunteer can either be active or inactive at a certain time point.\nMeanwhile, at each time point, a task arrives randomly, and the platform must decide whom to notify (activate) based on the likelihood of acceptance and the potential reward. This setup forms a basic RMAB model, with each volunteer representing an ‘arm’ of the bandit:\nthe state transitions of volunteers based on actions and associated probabilities. Active volunteers ($s_t = 1$), when no action is taken ($y_t = 0$) remains active. If action is taken ($y_t = 1$), they might generate a reward w.p. $p_i$ and become inactive, or stay active without generating a reward. Inactive volunteer return to activity based on specific probabilities ($q_i$).\nThe following figure illustrates a typical transition from $t$ to $t + 1$. Decision maker (platform) chooses a subset of volunteers to notify, and the volunteers generate rewards/transition according to their intrinsic probabilities $p_{i, k}, q_i$ and the current type $k$ of the job:\nfrom $t$ to $t + 1$, the platform’s action and associated transitions\nGreen Volunteers ($s_t = 1$): Represent active volunteers. When no action is taken ($y_t = 0$), the volunteer remains active in the next period ($s_{t+1} = 1$). When action is taken ($y_t = 1$), with probability $p_i$ a reward is generated and the volunteer transitions to an inactive state ($s_{t+1} = 0$). Otherwise, with probability $1 - p_i$, no reward is generated and the volunteer stays active. Red Volunteers ($s_t = 0$): Represent inactive volunteers. Regardless of action ($y_t = 0$), with probability $q_i$ the volunteer returns to active state in the next period ($s_{t+1} = 1$), and with probability $1 - q_i$​ remains inactive. Inactive volunteers generate no rewards. the Whittle Index Policy –– explained core concept The Whittle Index Policy assigns an index to each arm based on its current state, where the index almost represents the fair price for an arm $i$ when it’s being at state $x_i$. Denote the index as $w(x_i)$. The index represents a virtual price for the arm, at which we are indifferent between pulling it or not pulling it. The decision-maker then acts on the arms with the highest indices within the constraints of available resources.\nA crucial aspect of applying the Whittle Index is the property of indexability, which determines whether an arm’s index can be computed consistently as system dynamics evolve. Indexability is a property defined for an arm, and serves as the core engine for the Whittle Index Policy (or, more precisely, i’d rather call it as a heuristic).\ntheoretical foundations The seminal paper by Whittle (1988) introduced the index and demonstrated its application under conditions of resource constraints.\nWeber and Weiss (1990) extended this theory, showing that under certain conditions, using the Whittle Index is asymptotically optimal, meaning it performs nearly as well as the best possible policy as the number of arms and resources grows.\nThe RMAB framework and Whittle Index Policy find applications in various areas, including healthcare, like managing treatment schedules, and in telecommunications, where they allocate bandwidth among competing channels. Each application benefits from the model’s ability to handle dynamic environments efficiently.\nchallenges Despite its advantages, there is still challenges implementing the Whittle Index Policy in real-world scenarios. The biggest of these is the computational complexity involved in calculating the index for a large number of arms, particularly when the state transitions are complex.\nreferences Whittle, P. (1988). “Restless Bandits: Activity Allocation in a Changing World.” Journal of Applied Probability. Weber, R.R., \u0026 Weiss, G. (1990). “On an Index Policy for Restless Bandits.” Journal of Applied Probability. Feel free to delve deeper into the mathematical models discussed and explore how they can be tailored to specific applications in your field.\n",
  "wordCount" : "884",
  "inLanguage": "en",
  "datePublished": "2024-06-05T23:56:59+08:00",
  "dateModified": "2024-06-05T23:56:59+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/posts/whittle_index/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ariana's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "favicon.ico"
    }
  }
}
</script>
    <link rel="stylesheet" href="/css/custom.7bb5f7ae3acad2ea4ab78f919c1ccf795402cef69421e6a6ee01a51b03a733cb.css">
    
<script async src="https://www.googletagmanager.com/gtag/js?id=AW-16651897053">
</script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'AW-16651897053');
</script>

<script>
  gtag('event', 'conversion', {
      'send_to': 'AW-16651897053/wjItCK3FisUZEN2Rn4Q-',
      'value': 1.0,
      'currency': 'USD'
  });
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=AW-16651897053"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'AW-16651897053');
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="" accesskey="h" title="ariana&#39;s blog (Alt + H)">ariana&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="/search" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="archives" title="archive">
                    <span>archive</span>
                </a>
            </li>
            <li>
                <a href="posts/tags" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="/posts/pinned" title="about me">
                    <span>about me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      RMAB and Whittle index | a modern approach to decision-making
    </h1>
    <div class="post-meta"><span title='2024-06-05 23:56:59 +0800 +0800'>June 5, 2024</span>

</div>
  </header>
  <div class="post-content"><p>Decision-making in <strong>dynamic environments over time</strong> presents unique challenges, particularly when the conditions influencing decisions are constantly changing. In such scenarios, traditional decision-making models often fall short. This is where <strong>Restless Multi-Armed Bandits (RMAB)</strong> come into play. The method provides a robust framework for modeling and optimizing decisions over time. Here&rsquo;s a brief introduction of the concept of RMAB and the Whittle Index Policy.</p>
<h2 id="understanding-restless-multi-armed-bandits">understanding Restless Multi-Armed Bandits<a hidden class="anchor" aria-hidden="true" href="#understanding-restless-multi-armed-bandits">#</a></h2>
<h3 id="what-are-rmab">what are RMAB?<a hidden class="anchor" aria-hidden="true" href="#what-are-rmab">#</a></h3>
<p>The <strong>multi-armed bandit problem</strong> is a classic version of the problem of optimal allocation of activity under certainty: simply saying, one has $n$ projects, the state of project $i\in [n]$ being denoted by $x_i(t)$​.</p>
<p>One can operate only one project at once: if one operates project $i$ then one receives reward $g_i(x_i(t))$ in the time-interval $(t, t + 1)$ and the transition $x_i(t)\rightarrow x_i(t + 1)$ follows a Markov rule specific to project $i$. The unused projects neither yield reward nor change state; current states of all projects are known at any time.</p>
<p>The problem is to so choose the project at each moment that the expected discounted reward over an infinite future is maximal.</p>
<p><strong>Restless Multi-Armed Bandits</strong> extend this classic multi-armed bandit problem to scenarios where all projects (or, &lsquo;arms&rsquo;) evolve over time, regardless of whether they are chosen, and might incur reward regardless of if being activated. Moreover, one might be able to pull $B$ arms at any time. This dynamic nature makes RMAB particularly applicable to real-world decision-making processes, where neglecting any option can still lead to changes in its state.</p>
<h3 id="example">example<a hidden class="anchor" aria-hidden="true" href="#example">#</a></h3>
<p>Consider a platform coordinating volunteers for various tasks, where each volunteer can either be active or inactive at a certain time point.</p>
<p>Meanwhile, at each time point, a task arrives randomly, and the platform must decide whom to notify (activate) based on the likelihood of acceptance and the potential reward. This setup forms a basic RMAB model, with each volunteer representing an &lsquo;arm&rsquo; of the bandit:</p>
<figure class="align-center ">
    <img loading="lazy" src="/who_do_we_blame/volunteer_transition.jpeg#center"
         alt="the state transitions of volunteers based on actions and associated probabilities. Active volunteers ($s_t = 1$), when no action is taken ($y_t = 0$) remains active. If action is taken ($y_t = 1$), they might generate a reward w.p. $p_i$ and become inactive, or stay active without generating a reward. Inactive volunteer return to activity based on specific probabilities ($q_i$)."/> <figcaption>
            <p>the state transitions of volunteers based on actions and associated probabilities. Active volunteers ($s_t = 1$), when no action is taken ($y_t = 0$) remains active. If action is taken ($y_t = 1$), they might generate a reward w.p. $p_i$ and become inactive, or stay active without generating a reward. Inactive volunteer return to activity based on specific probabilities ($q_i$).</p>
        </figcaption>
</figure>

<p>The following figure illustrates a typical transition from $t$ to $t + 1$. Decision maker (platform) chooses a subset of volunteers to notify, and the volunteers generate rewards/transition according to their intrinsic probabilities $p_{i, k}, q_i$ and the current type $k$ of the job:</p>
<figure class="align-center ">
    <img loading="lazy" src="/who_do_we_blame/RMAB_volunteer.jpeg#center"
         alt="from $t$ to $t &#43; 1$, the platform&amp;rsquo;s action and associated transitions"/> <figcaption>
            <p>from $t$ to $t + 1$, the platform&rsquo;s action and associated transitions</p>
        </figcaption>
</figure>

<ul>
<li><strong>Green Volunteers ($s_t = 1$)</strong>: Represent active volunteers.
<ul>
<li>When no action is taken ($y_t = 0$), the volunteer remains active in the next period ($s_{t+1} = 1$).</li>
<li>When action is taken ($y_t = 1$), with probability $p_i$ a reward is generated and the volunteer transitions to an inactive state ($s_{t+1} = 0$). Otherwise, with probability $1 - p_i$, no reward is generated and the volunteer stays active.</li>
</ul>
</li>
<li><strong>Red Volunteers ($s_t = 0$)</strong>: Represent inactive volunteers.
<ul>
<li>Regardless of action ($y_t = 0$), with probability $q_i$ the volunteer returns to active state in the next period ($s_{t+1} = 1$), and with probability $1 - q_i$​ remains inactive.</li>
<li>Inactive volunteers generate no rewards.</li>
</ul>
</li>
</ul>
<h2 id="the-whittle-index-policy--explained">the Whittle Index Policy –– explained<a hidden class="anchor" aria-hidden="true" href="#the-whittle-index-policy--explained">#</a></h2>
<h3 id="core-concept">core concept<a hidden class="anchor" aria-hidden="true" href="#core-concept">#</a></h3>
<p>The Whittle Index Policy assigns an index to each arm based on its current state, where the index almost represents the <em><strong>fair price</strong></em> for an arm $i$ when it&rsquo;s being at state $x_i$. Denote the index as $w(x_i)$. The index represents a virtual price for the arm, at which we are indifferent between pulling it or not pulling it. The decision-maker then acts on the arms with the highest indices within the constraints of available resources.</p>
<p>A crucial aspect of applying the Whittle Index is the property of indexability, which determines whether an arm&rsquo;s index can be computed consistently as system dynamics evolve. <strong>Indexability</strong> is a property defined for an arm, and serves as the core engine for the Whittle Index Policy (or, more precisely, i&rsquo;d rather call it as a heuristic).</p>
<h3 id="theoretical-foundations">theoretical foundations<a hidden class="anchor" aria-hidden="true" href="#theoretical-foundations">#</a></h3>
<p>The seminal paper by Whittle (1988) introduced the index and demonstrated its application under conditions of resource constraints.</p>
<p>Weber and Weiss (1990) extended this theory, showing that under certain conditions, using the Whittle Index is asymptotically optimal, meaning it performs nearly as well as the best possible policy as the number of arms and resources grows.</p>
<p>The RMAB framework and Whittle Index Policy find applications in various areas, including healthcare, like managing treatment schedules, and in telecommunications, where they allocate bandwidth among competing channels. Each application benefits from the model&rsquo;s ability to handle dynamic environments <strong>efficiently</strong>.</p>
<h3 id="challenges">challenges<a hidden class="anchor" aria-hidden="true" href="#challenges">#</a></h3>
<p>Despite its advantages, there is still challenges implementing the Whittle Index Policy in real-world scenarios. The biggest of these is the computational complexity involved in calculating the index for a large number of arms, particularly when the state transitions are complex.</p>
<h2 id="references">references<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>Whittle, P. (1988). &ldquo;Restless Bandits: Activity Allocation in a Changing World.&rdquo; <em>Journal of Applied Probability.</em></li>
<li>Weber, R.R., &amp; Weiss, G. (1990). &ldquo;On an Index Policy for Restless Bandits.&rdquo; <em>Journal of Applied Probability.</em></li>
</ul>
<p>Feel free to delve deeper into the mathematical models discussed and explore how they can be tailored to specific applications in your field.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="">ariana&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script id="usercentrics-cmp" src="https://web.cmp.usercentrics.eu/ui/loader.js" data-settings-id="_KlbvhaKistzRA" async></script>

</body>

</html>
