<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bandits - background | ariana&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Consider a decision problem where you have a collection of $\mathcal L$ papers to read before your next week&rsquo;s meeting with your advisor. For each paper $l \in \mathcal L$, you can estimate its relevance and quality, represented as a potential intellectual reward $r_i$, modeled by a probability distribution $F_i(\cdot)$. $r_i$&rsquo;s value is only learnt and acquired after you thoroughly read through the paper. Additionally, each paper comes with an uncertain reading time $t_i$, modeled by another distribution $G_i(\cdot)$. Notably, you have the flexibility to start reading a paper, pause and switch to another, and return to it later, adapting your reading strategy as needed.">
<meta name="author" content="">
<link rel="canonical" href="//localhost:1313/posts/bandits/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="//localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/posts/bandits/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>




<meta property="og:title" content="Bandits - background" />
<meta property="og:description" content="Consider a decision problem where you have a collection of $\mathcal L$ papers to read before your next week&rsquo;s meeting with your advisor. For each paper $l \in \mathcal L$, you can estimate its relevance and quality, represented as a potential intellectual reward $r_i$, modeled by a probability distribution $F_i(\cdot)$. $r_i$&rsquo;s value is only learnt and acquired after you thoroughly read through the paper. Additionally, each paper comes with an uncertain reading time $t_i$, modeled by another distribution $G_i(\cdot)$. Notably, you have the flexibility to start reading a paper, pause and switch to another, and return to it later, adapting your reading strategy as needed." />
<meta property="og:type" content="article" />
<meta property="og:url" content="//localhost:1313/posts/bandits/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-04-24T19:22:10+08:00" />
<meta property="article:modified_time" content="2024-04-24T19:22:10+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Bandits - background"/>
<meta name="twitter:description" content="Consider a decision problem where you have a collection of $\mathcal L$ papers to read before your next week&rsquo;s meeting with your advisor. For each paper $l \in \mathcal L$, you can estimate its relevance and quality, represented as a potential intellectual reward $r_i$, modeled by a probability distribution $F_i(\cdot)$. $r_i$&rsquo;s value is only learnt and acquired after you thoroughly read through the paper. Additionally, each paper comes with an uncertain reading time $t_i$, modeled by another distribution $G_i(\cdot)$. Notably, you have the flexibility to start reading a paper, pause and switch to another, and return to it later, adapting your reading strategy as needed."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "//localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bandits - background",
      "item": "//localhost:1313/posts/bandits/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bandits - background",
  "name": "Bandits - background",
  "description": "Consider a decision problem where you have a collection of $\\mathcal L$ papers to read before your next week\u0026rsquo;s meeting with your advisor. For each paper $l \\in \\mathcal L$, you can estimate its relevance and quality, represented as a potential intellectual reward $r_i$, modeled by a probability distribution $F_i(\\cdot)$. $r_i$\u0026rsquo;s value is only learnt and acquired after you thoroughly read through the paper. Additionally, each paper comes with an uncertain reading time $t_i$, modeled by another distribution $G_i(\\cdot)$. Notably, you have the flexibility to start reading a paper, pause and switch to another, and return to it later, adapting your reading strategy as needed.\n",
  "keywords": [
    
  ],
  "articleBody": "Consider a decision problem where you have a collection of $\\mathcal L$ papers to read before your next week’s meeting with your advisor. For each paper $l \\in \\mathcal L$, you can estimate its relevance and quality, represented as a potential intellectual reward $r_i$, modeled by a probability distribution $F_i(\\cdot)$. $r_i$’s value is only learnt and acquired after you thoroughly read through the paper. Additionally, each paper comes with an uncertain reading time $t_i$, modeled by another distribution $G_i(\\cdot)$. Notably, you have the flexibility to start reading a paper, pause and switch to another, and return to it later, adapting your reading strategy as needed.\nThis scenario presents a sequential allocation problem, where you aim to optimize your reading schedule to maximize intellectual return. Such problems can be effectively modeled using a Markov decision process (MDP), a robust framework for handling decisions that unfold over time.\nIn general, in an MDP, one navigate through a series of states, with the probability of transitioning to the next state depending on the current state and the action taken–at each stage, one have several actions available, and the choice of action influences both the next state of the system and the immediate reward received. The objective is to devise a strategy that maximizes the total expected reward, which we refer to as the payoff. For now, we study MDP with an infinite time horizon where future rewards are discounted (to ensure that the sum of rewards remains finite).\nA Bandit process is a special type of Markov decision process. There’s a countable state space $E$ and space of action is to be chosen from a set of two controls ${0, 1}$. Consider at time $t$ where state is some $x_t\\in E$. Conventionally, control $0$ refers to freezing the state (taking no action, eg. not reading the paper) hence no reward is obtained, while control $1$ corresponds to the ‘continuation control’ (i.e. reading the paper) that yields reward $r(x_t)$ and triggers state transition to $y$ according to probability $P(y|x_t)$. A policy is any (possibly randomized) rule that for each decision time $t$, specifies the control ${0, 1}$ to be applied as a function of $t$, $x_t$ (the state at time $t$), the set of previous decisions and states. The most general policy that relies on all historical information but not the future ones is past- measurable. A policy which maximizes the expected total reward over the set of all policies for every initial state will be termed an optimal policy. Deterministic policies are those which involve no randomization. Stationary policies are those which involve no explicit time dependence. Markov policies are those for which the control chosen at time t depends only on $t$ and the state at time $t$. As for reward, it’s defined as the summation of discounted rewards over time, of course, viewed from period $t = 1$ at the beginning: $$ \\mathbb E \\sum_{i = 1}^{\\infty} a^t r(x_t) $$ Now consider a decision problem with $n$ available independent bandit processes $B_1, . . . , B_n$. The state of the decision process is the vector of states $ξ = (ξ_1,…,ξ_n)$ and is a member of the product space $E = E_1 \\times \\cdots \\times E_n$. If at decision time $t$ we choose to apply the continuation control to $B_i$, which is in state $ξ_i$, then a reward $r_i(ξ_i)$ is obtained from it. Standard dynamic programming gives us the solution over all past-mesuarable policies $\\pi$: $$ \\begin{align} V(\\xi) \u0026 = \\sup_{\\pi}\\mathbb E\\left[ \\sum_{t = 0}^\\infty a^t r_{i_t}(\\xi_{i_t}(t)) \\bigg | \\xi(0) = \\xi \\right]\\cr \u0026 = \\max_{i\\in [n]}{r_i(\\xi_i) + a\\sum_{y\\in E_i}P_i(y|\\xi_i)V(y, \\xi_{-i})} \\end{align} $$ btw, $\\xi_{-i} = [\\xi_1, \\ldots, \\xi_{i - 1}, \\xi_{i + 1}, \\ldots, \\xi_n]$. And the above process, unfortunately, is computationally infeasible for even moderate sizes of $|E|$ and $n$​.\n— stay tuned for next episode - Gittin’s index.\n",
  "wordCount" : "640",
  "inLanguage": "en",
  "datePublished": "2024-04-24T19:22:10+08:00",
  "dateModified": "2024-04-24T19:22:10+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/posts/bandits/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ariana's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/favicon.ico"
    }
  }
}
</script>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Product+Sans:wght@400;700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="//localhost:1313/css/custom.b017fc6c61ff4eacd2943e3c9726e1a935c706d1ea1a1b7c17773e63b47342d3.css">
    
<script async src="https://www.googletagmanager.com/gtag/js?id=AW-16651897053">
</script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'AW-16651897053');
</script>

<script>
  gtag('event', 'conversion', {
      'send_to': 'AW-16651897053/wjItCK3FisUZEN2Rn4Q-',
      'value': 1.0,
      'currency': 'USD'
  });
</script>


<script async src="https://www.googletagmanager.com/gtag/js?id=AW-16651897053"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'AW-16651897053');
</script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="ariana&#39;s blog (Alt + H)">ariana&#39;s blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/search" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/archives" title="archive">
                    <span>archive</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/posts/tags" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/posts/pinned" title="about me">
                    <span>about me</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/procrastination_bulletin" title="pending ideas">
                    <span>pending ideas</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Bandits - background
    </h1>
    <div class="post-meta">&lt;span title=&#39;2024-04-24 19:22:10 &#43;0800 &#43;0800&#39;&gt;April 24, 2024&lt;/span&gt;

</div>
  </header>
  <div class="post-content"><p>Consider a decision problem where you have a collection of $\mathcal L$ papers to read before your next week&rsquo;s meeting with your advisor. For each paper $l \in \mathcal L$, you can estimate its relevance and quality, represented as a potential intellectual reward $r_i$, modeled by a probability distribution $F_i(\cdot)$. $r_i$&rsquo;s value is only learnt and acquired after you thoroughly read through the paper. Additionally, each paper comes with an uncertain reading time $t_i$, modeled by another distribution $G_i(\cdot)$. Notably, you have the flexibility to start reading a paper, pause and switch to another, and return to it later, adapting your reading strategy as needed.</p>
<p>This scenario presents a sequential allocation problem, where you aim to optimize your reading schedule to maximize intellectual return. Such problems can be effectively modeled using a <strong>Markov decision process (MDP)</strong>, a robust framework for handling decisions that unfold over time.</p>
<p>In general, in an MDP, one navigate through a series of states, with the probability of transitioning to the next state depending on the current state and the action taken–at each stage, one have several actions available, and the choice of action influences both the next state of the system and the immediate reward received. The objective is to devise a strategy that maximizes the total expected reward, which we refer to as the <em>payoff</em>. For now, we study MDP with an infinite time horizon where future rewards are discounted (to ensure that the sum of rewards remains finite).</p>
<p>A <em><strong>Bandit process</strong></em> is a special type of Markov decision process. There&rsquo;s a <em>countable state space $E$</em> and space of <em><strong>action</strong></em> is to be chosen from a set of two <em><strong>controls</strong></em> ${0, 1}$. Consider at time $t$ where state is some $x_t\in E$. Conventionally, control $0$ refers to <em>freezing</em> the state (taking no action, eg. not reading the paper) hence no reward is obtained, while control $1$ corresponds to the &lsquo;<em>continuation</em> control&rsquo; (i.e. reading the paper) that yields reward $r(x_t)$ and triggers state transition to $y$ according to probability $P(y|x_t)$. A <em>policy</em> is any (possibly randomized) rule that for each decision time $t$, specifies the control ${0, 1}$ to be applied as a function of $t$, $x_t$ (the state at time $t$), the set of previous decisions and states. The most general policy that relies on all historical information but not the future ones is <em>past- measurable</em>. A policy which maximizes the expected total reward over the set of all policies for every initial state will be termed an <em>optimal</em> policy. <em>Deterministic</em> policies are those which involve no randomization. <em>Stationary</em> policies are those which involve no explicit time dependence. <em>Markov</em> policies are those for which the control chosen at time t depends only on $t$ and the state at time $t$. As for reward, it&rsquo;s defined as the summation of discounted rewards over time, of course, viewed from period $t = 1$ at the beginning:
$$
\mathbb E \sum_{i = 1}^{\infty} a^t r(x_t)
$$
Now consider a decision problem with $n$ available independent bandit processes $B_1, . . . , B_n$. The state of the decision process is the vector of states $ξ = (ξ_1,&hellip;,ξ_n)$ and is a member of the product space $E = E_1 \times \cdots \times E_n$. If at decision time $t$ we choose to apply the continuation control to $B_i$, which is in state $ξ_i$, then a reward $r_i(ξ_i)$ is obtained from it. Standard dynamic programming gives us the solution over all past-mesuarable policies $\pi$:
$$
\begin{align}
V(\xi) &amp; = \sup_{\pi}\mathbb E\left[ \sum_{t = 0}^\infty a^t r_{i_t}(\xi_{i_t}(t)) \bigg | \xi(0) = \xi \right]\cr
&amp; = \max_{i\in [n]}{r_i(\xi_i) + a\sum_{y\in E_i}P_i(y|\xi_i)V(y, \xi_{-i})}
\end{align}
$$
btw, $\xi_{-i} = [\xi_1, \ldots, \xi_{i - 1}, \xi_{i + 1}, \ldots, \xi_n]$. And the above process, unfortunately, is computationally infeasible for even moderate sizes of $|E|$ and $n$​.</p>
<p>&mdash; stay tuned for next episode - Gittin&rsquo;s index.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="//localhost:1313/">ariana&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script id="usercentrics-cmp" src="https://web.cmp.usercentrics.eu/ui/loader.js" data-settings-id="_KlbvhaKistzRA" async></script>

</body>

</html>
